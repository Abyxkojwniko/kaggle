{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42de823f-c09a-4024-a83e-0f9395161a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d0d02f0-73ee-485a-8c22-be6da99aa0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import tokenizers\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba9bd009-6b3c-410b-a42f-1aa6609e18da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    print_freq = 500\n",
    "    num_workers = 4\n",
    "    model = \"xlm-roberta-base\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    gradient_checkpointing = False\n",
    "    num_cycles = 0.5\n",
    "    warmup_ratio = 0.1\n",
    "    epochs = 1\n",
    "    encoder_lr = 1e-5\n",
    "    decoder_lr = 1e-4\n",
    "    eps = 1e-6\n",
    "    betas = (0.9, 0.999)\n",
    "    batch_size = 32\n",
    "    weight_decay = 0.01\n",
    "    max_grad_norm = 0.012\n",
    "    max_len = 512\n",
    "    n_folds = 5\n",
    "    seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31ba5498-8892-4166-8261-14cd3a69eaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(cfg):\n",
    "    random.seed(cfg.seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(cfg.seed)\n",
    "    np.random.seed(cfg.seed)\n",
    "    torch.manual_seed(cfg.seed)\n",
    "    torch.cuda.manual_seed(cfg.seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "#这一个函数应该就是设置种子，保证实验结果可复现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad06c2b3-7022-47d8-a557-01a20523cacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2_score(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))#去除x中重复的的元素\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    tp = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n",
    "    fp = np.array([len(x[1] - x[0]) for x in zip(y_true, y_pred)])\n",
    "    fn = np.array([len(x[0] - x[1]) for x in zip(y_true, y_pred)])\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f2 = tp / (tp + 0.2 * fp + 0.8 * fn)\n",
    "    return round(f2.mean(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40c9cbf5-fe95-43f9-91f2-0a8ad1a3059b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(cfg):\n",
    "    train = pd.read_csv('/root/autodl-tmp/train.csv')\n",
    "    train['title1'].fillna(\"Title does not exist\", inplace = True)\n",
    "    train['title2'].fillna(\"Title does not exist\", inplace = True)\n",
    "    correlations = pd.read_csv('/root/autodl-tmp/correlations.csv')\n",
    "    # Create feature column\n",
    "    train['text'] = train['title1'] + '[SEP]' + train['title2']\n",
    "    print(' ')\n",
    "    print('-' * 50)\n",
    "    print(f\"train.shape: {train.shape}\")\n",
    "    print(f\"correlations.shape: {correlations.shape}\")\n",
    "    return train, correlations\n",
    "#读入数据，title1和title2是topics和content中的title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08366d32-e3c8-487d-bc18-c678e042d6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_split(train, cfg):\n",
    "    kfold = StratifiedGroupKFold(n_splits = cfg.n_folds, shuffle = True, random_state = cfg.seed)\n",
    "    for num, (train_index, val_index) in enumerate(kfold.split(train, train['target'], train['topics_ids'])):#enumerate函数用于一个可遍历的数据对象组合为一个索引序列，同时列出数据和数据下标\n",
    "        train.loc[val_index, 'fold'] = int(num)\n",
    "    train['fold'] = train['fold'].astype(int)\n",
    "    return train\n",
    "#这个函数加入了fold和text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cda8ec90-3ffd-478d-b7d5-0587414b3498",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_length(train, cfg):\n",
    "    lengths = []\n",
    "    for text in tqdm(train['text'].fillna(\"\").values, total = len(train)):\n",
    "        length = len(cfg.tokenizer(text, add_special_tokens = False)['input_ids'])\n",
    "        lengths.append(length)\n",
    "    cfg.max_len = max(lengths) + 2 # cls & sep\n",
    "    print(f\"max_len: {cfg.max_len}\")\n",
    "#这个函数没怎么看懂是什么意思"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e767eaba-af46-43f1-b62b-3bcc60014bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(text, cfg):\n",
    "    inputs = cfg.tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors = None, \n",
    "        add_special_tokens = True, \n",
    "        max_length = cfg.max_len,\n",
    "        pad_to_max_length = True,\n",
    "        truncation = True\n",
    "    )#主要是不知道这些参数具体表示了什么\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype = torch.long)\n",
    "    return inputs\n",
    "#问题同上，最后进行预测同样要对text的数据进行处理，那么就是现将topic和content的title相加处理为text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59c51da3-a1c2-4d3d-b6f1-ab7549398528",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_dataset(Dataset):\n",
    "    def __init__(self, df, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['text'].values\n",
    "        self.labels = df['target'].values\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_input(self.texts[item], self.cfg)\n",
    "        label = torch.tensor(self.labels[item], dtype = torch.float)\n",
    "        return inputs, label\n",
    "#将得到的text，target标签下的进行处理和输入，target这个需要knn进行处理，但是在使用模型预测阶段不需要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4204ba3-3f4f-4f79-97a6-0797419485d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(inputs):\n",
    "    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max())\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = inputs[k][:,:mask_len]\n",
    "    return inputs\n",
    "#这里对text进行注意力处理，做预测的时候输入数据也应该做这个处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57c50d29-672a-4e30-9bd6-3db9597ab386",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "#同样是一个池化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca313f81-f350-4475-8435-a90a634690d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states = True)\n",
    "        self.config.hidden_dropout = 0.0\n",
    "        self.config.hidden_dropout_prob = 0.0\n",
    "        self.config.attention_dropout = 0.0\n",
    "        self.config.attention_probs_dropout_prob = 0.0\n",
    "        self.model = AutoModel.from_pretrained(cfg.model, config = self.config)\n",
    "        if self.cfg.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        self.pool = MeanPooling()\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 1)\n",
    "        self._init_weights(self.fc)\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        feature = self.pool(last_hidden_state, inputs['attention_mask'])\n",
    "        return feature\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(feature)\n",
    "        return output\n",
    "    #训练，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b212c192-c20e-496f-b322-f455b4644dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "#这个函数没懂在做什么，主要是参数含义不清楚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46e45d40-df2a-46a9-87c5-2ccc3818c886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "#训练计时"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35052c85-1d33-4ea5-9345-6a61ef3494ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device, cfg):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled = True)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, (inputs, target) in enumerate(train_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        target = target.to(device)\n",
    "        batch_size = target.size(0)\n",
    "        with torch.cuda.amp.autocast(enabled = True):\n",
    "            y_preds = model(inputs)\n",
    "            loss = criterion(y_preds.view(-1), target)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_grad_norm)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "        scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % cfg.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch + 1, \n",
    "                          step, \n",
    "                          len(train_loader), \n",
    "                          remain = timeSince(start, float(step + 1) / len(train_loader)),\n",
    "                          loss = losses,\n",
    "                          grad_norm = grad_norm,\n",
    "                          lr = scheduler.get_lr()[0]))\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3addd8b-8efb-452d-9e39-da6a31b802e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fn(valid_loader, model, criterion, device, cfg):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    start = end = time.time()\n",
    "    for step, (inputs, target) in enumerate(valid_loader):\n",
    "        inputs = collate(inputs)\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        target = target.to(device)\n",
    "        batch_size = target.size(0)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        loss = criterion(y_preds.view(-1), target)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.sigmoid().squeeze().to('cpu').numpy().reshape(-1))\n",
    "        end = time.time()\n",
    "        if step % cfg.print_freq == 0 or step == (len(valid_loader) - 1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, \n",
    "                          len(valid_loader),\n",
    "                          loss = losses,\n",
    "                          remain = timeSince(start, float(step + 1) / len(valid_loader))))\n",
    "    predictions = np.concatenate(preds, axis = 0)\n",
    "    return losses.avg, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e39d17e-affa-418f-b03f-c67b6ffd31ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_threshold(x_val, val_predictions, correlations):\n",
    "    best_score = 0\n",
    "    best_threshold = None\n",
    "    for thres in np.arange(0.001, 0.1, 0.001):\n",
    "        x_val['predictions'] = np.where(val_predictions > thres, 1, 0)\n",
    "        x_val1 = x_val[x_val['predictions'] == 1]\n",
    "        x_val1 = x_val1.groupby(['topics_ids'])['content_ids'].unique().reset_index()\n",
    "        x_val1['content_ids'] = x_val1['content_ids'].apply(lambda x: ' '.join(x))\n",
    "        x_val1.columns = ['topic_id', 'predictions']\n",
    "        x_val0 = pd.Series(x_val['topics_ids'].unique())\n",
    "        x_val0 = x_val0[~x_val0.isin(x_val1['topic_id'])]\n",
    "        x_val0 = pd.DataFrame({'topic_id': x_val0.values, 'predictions': \"\"})\n",
    "        x_val_r = pd.concat([x_val1, x_val0], axis = 0, ignore_index = True)\n",
    "        x_val_r = x_val_r.merge(correlations, how = 'left', on = 'topic_id')\n",
    "        score = f2_score(x_val_r['content_ids'], x_val_r['predictions'])\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_threshold = thres\n",
    "    return best_score, best_threshold\n",
    "    #选了一个最好的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f85e134-e1e2-4406-abd3-c4fe61db558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_one_fold(train, correlations, fold, cfg):\n",
    "    print(' ')\n",
    "    print(f\"========== fold: {fold} training ==========\")\n",
    "    # Split train & validation\n",
    "    x_train = train[train['fold'] != fold]\n",
    "    x_val = train[train['fold'] == fold]\n",
    "    valid_labels = x_val['target'].values\n",
    "    train_dataset = custom_dataset(x_train, cfg)\n",
    "    valid_dataset = custom_dataset(x_val, cfg)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size = cfg.batch_size, \n",
    "        shuffle = True, \n",
    "        num_workers = cfg.num_workers, \n",
    "        pin_memory = True, \n",
    "        drop_last = True\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size = cfg.batch_size, \n",
    "        shuffle = False, \n",
    "        num_workers = cfg.num_workers, \n",
    "        pin_memory = True, \n",
    "        drop_last = False\n",
    "    )\n",
    "    # Get model\n",
    "    model = custom_model(cfg)\n",
    "    model.to(device)\n",
    "    # Optimizer\n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay = 0.0):\n",
    "        no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "            'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "            'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n",
    "            'lr': decoder_lr, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "    optimizer_parameters = get_optimizer_params(\n",
    "        model, \n",
    "        encoder_lr = cfg.encoder_lr, \n",
    "        decoder_lr = cfg.decoder_lr,\n",
    "        weight_decay = cfg.weight_decay\n",
    "    )\n",
    "    optimizer = AdamW(\n",
    "        optimizer_parameters, \n",
    "        lr = cfg.encoder_lr, \n",
    "        eps = cfg.eps, \n",
    "        betas = cfg.betas\n",
    "    )\n",
    "    num_train_steps = int(len(x_train) / cfg.batch_size * cfg.epochs)\n",
    "    num_warmup_steps = num_train_steps * cfg.warmup_ratio\n",
    "    # Scheduler\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps = num_warmup_steps, \n",
    "        num_training_steps = num_train_steps, \n",
    "        num_cycles = cfg.num_cycles\n",
    "        )\n",
    "    # Training & Validation loop\n",
    "    criterion = nn.BCEWithLogitsLoss(reduction = \"mean\")\n",
    "    best_score = 0\n",
    "    for epoch in range(cfg.epochs):\n",
    "        start_time = time.time()\n",
    "        # Train\n",
    "        avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device, cfg)\n",
    "        # Validation\n",
    "        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device, cfg)\n",
    "        # Compute f2_score\n",
    "        score, threshold = get_best_threshold(x_val, predictions, correlations)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        print(f'Epoch {epoch+1} - Score: {score:.4f} - Threshold: {threshold:.5f}')\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            print(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save(\n",
    "                {'model': model.state_dict(), 'predictions': predictions}, \n",
    "                f\"{cfg.model.replace('/', '-')}_fold{fold}_{cfg.seed}.pth\"\n",
    "                )\n",
    "            val_predictions = predictions\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    # Get best threshold\n",
    "    best_score, best_threshold = get_best_threshold(x_val, val_predictions, correlations)\n",
    "    print(f'Our CV score is {best_score} using a threshold of {best_threshold}')\n",
    "#这里具体就是训练的过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a346972f-72b4-4ae0-827b-7595f3d21162",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad42ad4a-afa8-452b-9780-06fa6fb00383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "train.shape: (615170, 6)\n",
      "correlations.shape: (61517, 2)\n"
     ]
    }
   ],
   "source": [
    "train, correlations = read_data(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0b7b5b8-dd94-48c7-95d6-ad70b03b3f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topics_ids</th>\n",
       "      <th>content_ids</th>\n",
       "      <th>title1</th>\n",
       "      <th>title2</th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_3d9ad9931021</td>\n",
       "      <td>c_efb73ad83f4b</td>\n",
       "      <td>Title does not exist</td>\n",
       "      <td>Title does not exist</td>\n",
       "      <td>0</td>\n",
       "      <td>Title does not exist[SEP]Title does not exist</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_3d9ad9931021</td>\n",
       "      <td>c_159f205b73db</td>\n",
       "      <td>Title does not exist</td>\n",
       "      <td>Title does not exist</td>\n",
       "      <td>0</td>\n",
       "      <td>Title does not exist[SEP]Title does not exist</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_3d9ad9931021</td>\n",
       "      <td>c_77574ef20c1f</td>\n",
       "      <td>Title does not exist</td>\n",
       "      <td>Title does not exist</td>\n",
       "      <td>0</td>\n",
       "      <td>Title does not exist[SEP]Title does not exist</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_3d9ad9931021</td>\n",
       "      <td>c_a04562126266</td>\n",
       "      <td>Title does not exist</td>\n",
       "      <td>Title does not exist</td>\n",
       "      <td>0</td>\n",
       "      <td>Title does not exist[SEP]Title does not exist</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_3d9ad9931021</td>\n",
       "      <td>c_77105b4b84cc</td>\n",
       "      <td>Title does not exist</td>\n",
       "      <td>Title does not exist</td>\n",
       "      <td>0</td>\n",
       "      <td>Title does not exist[SEP]Title does not exist</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615165</th>\n",
       "      <td>t_70da08637930</td>\n",
       "      <td>c_eb6448437b5f</td>\n",
       "      <td>8.1.5 Use dot (.) and cross (x) diagrams to il...</td>\n",
       "      <td>Level 2: Describe the formation of ionic bond ...</td>\n",
       "      <td>0</td>\n",
       "      <td>8.1.5 Use dot (.) and cross (x) diagrams to il...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615166</th>\n",
       "      <td>t_70da08637930</td>\n",
       "      <td>c_07c1da15995b</td>\n",
       "      <td>8.1.5 Use dot (.) and cross (x) diagrams to il...</td>\n",
       "      <td>Level 3: Describe the formation of ionic bond ...</td>\n",
       "      <td>0</td>\n",
       "      <td>8.1.5 Use dot (.) and cross (x) diagrams to il...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615167</th>\n",
       "      <td>t_70da08637930</td>\n",
       "      <td>c_17ff16d31106</td>\n",
       "      <td>8.1.5 Use dot (.) and cross (x) diagrams to il...</td>\n",
       "      <td>Level 1: Describe the formation of ionic bond ...</td>\n",
       "      <td>0</td>\n",
       "      <td>8.1.5 Use dot (.) and cross (x) diagrams to il...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615168</th>\n",
       "      <td>t_70da08637930</td>\n",
       "      <td>c_7cb9a57f2219</td>\n",
       "      <td>8.1.5 Use dot (.) and cross (x) diagrams to il...</td>\n",
       "      <td>Ligações iônicas, covalentes e metálicas</td>\n",
       "      <td>0</td>\n",
       "      <td>8.1.5 Use dot (.) and cross (x) diagrams to il...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>615169</th>\n",
       "      <td>t_70da08637930</td>\n",
       "      <td>c_9573a6ec9ae8</td>\n",
       "      <td>8.1.5 Use dot (.) and cross (x) diagrams to il...</td>\n",
       "      <td>Électronégativité et liaison chimique</td>\n",
       "      <td>0</td>\n",
       "      <td>8.1.5 Use dot (.) and cross (x) diagrams to il...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>615170 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            topics_ids     content_ids  \\\n",
       "0       t_3d9ad9931021  c_efb73ad83f4b   \n",
       "1       t_3d9ad9931021  c_159f205b73db   \n",
       "2       t_3d9ad9931021  c_77574ef20c1f   \n",
       "3       t_3d9ad9931021  c_a04562126266   \n",
       "4       t_3d9ad9931021  c_77105b4b84cc   \n",
       "...                ...             ...   \n",
       "615165  t_70da08637930  c_eb6448437b5f   \n",
       "615166  t_70da08637930  c_07c1da15995b   \n",
       "615167  t_70da08637930  c_17ff16d31106   \n",
       "615168  t_70da08637930  c_7cb9a57f2219   \n",
       "615169  t_70da08637930  c_9573a6ec9ae8   \n",
       "\n",
       "                                                   title1  \\\n",
       "0                                    Title does not exist   \n",
       "1                                    Title does not exist   \n",
       "2                                    Title does not exist   \n",
       "3                                    Title does not exist   \n",
       "4                                    Title does not exist   \n",
       "...                                                   ...   \n",
       "615165  8.1.5 Use dot (.) and cross (x) diagrams to il...   \n",
       "615166  8.1.5 Use dot (.) and cross (x) diagrams to il...   \n",
       "615167  8.1.5 Use dot (.) and cross (x) diagrams to il...   \n",
       "615168  8.1.5 Use dot (.) and cross (x) diagrams to il...   \n",
       "615169  8.1.5 Use dot (.) and cross (x) diagrams to il...   \n",
       "\n",
       "                                                   title2  target  \\\n",
       "0                                    Title does not exist       0   \n",
       "1                                    Title does not exist       0   \n",
       "2                                    Title does not exist       0   \n",
       "3                                    Title does not exist       0   \n",
       "4                                    Title does not exist       0   \n",
       "...                                                   ...     ...   \n",
       "615165  Level 2: Describe the formation of ionic bond ...       0   \n",
       "615166  Level 3: Describe the formation of ionic bond ...       0   \n",
       "615167  Level 1: Describe the formation of ionic bond ...       0   \n",
       "615168           Ligações iônicas, covalentes e metálicas       0   \n",
       "615169              Électronégativité et liaison chimique       0   \n",
       "\n",
       "                                                     text  fold  \n",
       "0           Title does not exist[SEP]Title does not exist     2  \n",
       "1           Title does not exist[SEP]Title does not exist     2  \n",
       "2           Title does not exist[SEP]Title does not exist     2  \n",
       "3           Title does not exist[SEP]Title does not exist     2  \n",
       "4           Title does not exist[SEP]Title does not exist     2  \n",
       "...                                                   ...   ...  \n",
       "615165  8.1.5 Use dot (.) and cross (x) diagrams to il...     2  \n",
       "615166  8.1.5 Use dot (.) and cross (x) diagrams to il...     2  \n",
       "615167  8.1.5 Use dot (.) and cross (x) diagrams to il...     2  \n",
       "615168  8.1.5 Use dot (.) and cross (x) diagrams to il...     2  \n",
       "615169  8.1.5 Use dot (.) and cross (x) diagrams to il...     2  \n",
       "\n",
       "[615170 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_split(train, CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fa1202f-5352-47b1-8c37-00cc3ac1060f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ef63e6ada314b4881b8ab3454d6c8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/615170 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len: 172\n"
     ]
    }
   ],
   "source": [
    "get_max_length(train, CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "751e1239-f713-49c5-87e9-b517c87d0ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3972abf6eaa0445593b0451e3187069d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/15377] Elapsed 0m 0s (remain 179m 25s) Loss: 0.8633(0.8633) Grad: inf  LR: 0.00000001  \n",
      "Epoch: [1][500/15377] Elapsed 2m 22s (remain 70m 20s) Loss: 0.1463(0.5238) Grad: 4.6353  LR: 0.00000326  \n",
      "Epoch: [1][1000/15377] Elapsed 4m 44s (remain 68m 3s) Loss: 0.1230(0.3685) Grad: 6.7838  LR: 0.00000651  \n",
      "Epoch: [1][1500/15377] Elapsed 7m 4s (remain 65m 25s) Loss: 0.1427(0.3115) Grad: 6.4889  LR: 0.00000976  \n",
      "Epoch: [1][2000/15377] Elapsed 9m 26s (remain 63m 8s) Loss: 0.1852(0.2805) Grad: 15.6007  LR: 0.00000997  \n",
      "Epoch: [1][2500/15377] Elapsed 11m 50s (remain 60m 55s) Loss: 0.1994(0.2591) Grad: 6.5243  LR: 0.00000988  \n",
      "Epoch: [1][3000/15377] Elapsed 14m 11s (remain 58m 32s) Loss: 0.3075(0.2446) Grad: 7.0194  LR: 0.00000973  \n",
      "Epoch: [1][3500/15377] Elapsed 16m 34s (remain 56m 12s) Loss: 0.3481(0.2336) Grad: 7.5603  LR: 0.00000951  \n",
      "Epoch: [1][4000/15377] Elapsed 18m 56s (remain 53m 49s) Loss: 0.4011(0.2249) Grad: 17.3921  LR: 0.00000924  \n",
      "Epoch: [1][4500/15377] Elapsed 21m 18s (remain 51m 28s) Loss: 0.1802(0.2170) Grad: 9.9259  LR: 0.00000891  \n",
      "Epoch: [1][5000/15377] Elapsed 23m 39s (remain 49m 5s) Loss: 0.1270(0.2102) Grad: 4.1117  LR: 0.00000853  \n",
      "Epoch: [1][5500/15377] Elapsed 26m 1s (remain 46m 42s) Loss: 0.1647(0.2045) Grad: 7.8092  LR: 0.00000811  \n",
      "Epoch: [1][6000/15377] Elapsed 28m 23s (remain 44m 21s) Loss: 0.0381(0.1994) Grad: 8.9298  LR: 0.00000765  \n",
      "Epoch: [1][6500/15377] Elapsed 30m 46s (remain 42m 0s) Loss: 0.0743(0.1954) Grad: 3.1867  LR: 0.00000715  \n",
      "Epoch: [1][7000/15377] Elapsed 33m 6s (remain 39m 36s) Loss: 0.0686(0.1918) Grad: 3.9020  LR: 0.00000662  \n",
      "Epoch: [1][7500/15377] Elapsed 35m 28s (remain 37m 14s) Loss: 0.0667(0.1885) Grad: 6.6361  LR: 0.00000608  \n",
      "Epoch: [1][8000/15377] Elapsed 37m 49s (remain 34m 52s) Loss: 0.0634(0.1859) Grad: 2.7944  LR: 0.00000552  \n",
      "Epoch: [1][8500/15377] Elapsed 40m 12s (remain 32m 31s) Loss: 0.1956(0.1830) Grad: 12.2665  LR: 0.00000495  \n",
      "Epoch: [1][9000/15377] Elapsed 42m 37s (remain 30m 11s) Loss: 0.3736(0.1807) Grad: 9.2326  LR: 0.00000438  \n",
      "Epoch: [1][9500/15377] Elapsed 44m 59s (remain 27m 49s) Loss: 0.1718(0.1783) Grad: 7.8753  LR: 0.00000383  \n",
      "Epoch: [1][10000/15377] Elapsed 47m 21s (remain 25m 27s) Loss: 0.1939(0.1762) Grad: 7.6875  LR: 0.00000328  \n",
      "Epoch: [1][10500/15377] Elapsed 49m 43s (remain 23m 5s) Loss: 0.0800(0.1741) Grad: 3.1154  LR: 0.00000276  \n",
      "Epoch: [1][11000/15377] Elapsed 52m 5s (remain 20m 43s) Loss: 0.1694(0.1722) Grad: 4.8383  LR: 0.00000227  \n",
      "Epoch: [1][11500/15377] Elapsed 54m 27s (remain 18m 21s) Loss: 0.0646(0.1703) Grad: 4.0737  LR: 0.00000181  \n",
      "Epoch: [1][12000/15377] Elapsed 56m 49s (remain 15m 59s) Loss: 0.1821(0.1687) Grad: 7.6332  LR: 0.00000140  \n",
      "Epoch: [1][12500/15377] Elapsed 59m 12s (remain 13m 37s) Loss: 0.2444(0.1669) Grad: 8.1563  LR: 0.00000103  \n",
      "Epoch: [1][13000/15377] Elapsed 61m 33s (remain 11m 14s) Loss: 0.0510(0.1653) Grad: 6.7091  LR: 0.00000071  \n",
      "Epoch: [1][13500/15377] Elapsed 63m 55s (remain 8m 52s) Loss: 0.0444(0.1640) Grad: 3.7519  LR: 0.00000045  \n",
      "Epoch: [1][14000/15377] Elapsed 66m 19s (remain 6m 31s) Loss: 0.1680(0.1625) Grad: 13.0003  LR: 0.00000024  \n",
      "Epoch: [1][14500/15377] Elapsed 68m 41s (remain 4m 8s) Loss: 0.0387(0.1612) Grad: 2.3253  LR: 0.00000010  \n",
      "Epoch: [1][15000/15377] Elapsed 71m 3s (remain 1m 46s) Loss: 0.1028(0.1601) Grad: 10.5849  LR: 0.00000002  \n",
      "Epoch: [1][15376/15377] Elapsed 72m 50s (remain 0m 0s) Loss: 0.0426(0.1592) Grad: 3.8829  LR: 0.00000000  \n",
      "EVAL: [0/3847] Elapsed 0m 0s (remain 23m 13s) Loss: 0.0648(0.0648) \n",
      "EVAL: [500/3847] Elapsed 0m 12s (remain 1m 26s) Loss: 0.0021(0.0728) \n",
      "EVAL: [1000/3847] Elapsed 0m 26s (remain 1m 15s) Loss: 0.2808(0.0851) \n",
      "EVAL: [1500/3847] Elapsed 0m 42s (remain 1m 6s) Loss: 0.2107(0.1040) \n",
      "EVAL: [2000/3847] Elapsed 0m 59s (remain 0m 55s) Loss: 0.1090(0.1141) \n",
      "EVAL: [2500/3847] Elapsed 1m 18s (remain 0m 42s) Loss: 0.2857(0.1215) \n",
      "EVAL: [3000/3847] Elapsed 1m 38s (remain 0m 27s) Loss: 0.1565(0.1262) \n",
      "EVAL: [3500/3847] Elapsed 2m 0s (remain 0m 11s) Loss: 0.1556(0.1297) \n",
      "EVAL: [3846/3847] Elapsed 2m 19s (remain 0m 0s) Loss: 0.5269(0.1304) \n",
      "Epoch 1 - avg_train_loss: 0.1592  avg_val_loss: 0.1304  time: 4616s\n",
      "Epoch 1 - Score: 0.2975 - Threshold: 0.03800\n",
      "Epoch 1 - Save Best Score: 0.2975 Model\n",
      "Our CV score is 0.2975 using a threshold of 0.038\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate_one_fold(train, correlations, 0, CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "68df3c8f-f2b4-4213-ba1f-b182f0af73be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01c7558-aeb1-4c71-9b96-68db0c3eb2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
