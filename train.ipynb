{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a46be611-22eb-4b84-b4d9-6b1a39677c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3550889a",
   "metadata": {},
   "source": [
    "这个代码集合了前面knn训练的过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce651f8e-69e8-450c-8435-4e7b76a5d867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import tokenizers\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
    "import cupy as cp\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63c22a84-444e-4225-9ce5-3f12b1ef74be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    print_freq = 3000\n",
    "    num_workers = 4\n",
    "    uns_model = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
    "    sup_model = \"xlm-roberta-base\"\n",
    "    uns_tokenizer = AutoTokenizer.from_pretrained(uns_model)\n",
    "    sup_tokenizer = AutoTokenizer.from_pretrained(sup_model)\n",
    "    gradient_checkpointing = False\n",
    "    batch_size = 32\n",
    "    n_folds = 5\n",
    "    top_n = 1000\n",
    "    seed = 42\n",
    "    threshold = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "037d0281-6fe4-415f-92c9-36bd45d904a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(cfg):\n",
    "    topics = pd.read_csv('/root/autodl-tmp/topics.csv')\n",
    "    content = pd.read_csv('/root/autodl-tmp/content.csv')\n",
    "    sample_submission = pd.read_csv('/root/autodl-tmp/sample_submission.csv')\n",
    "    # Merge topics with sample submission to only infer test topics\n",
    "    topics = topics.merge(sample_submission, how = 'inner', left_on = 'id', right_on = 'topic_id')\n",
    "    # Fillna titles\n",
    "    topics['title'].fillna(\"\", inplace = True)\n",
    "    content['title'].fillna(\"\", inplace = True)\n",
    "    # Sort by title length to make inference faster\n",
    "    topics['length'] = topics['title'].apply(lambda x: len(x))\n",
    "    content['length'] = content['title'].apply(lambda x: len(x))\n",
    "    topics.sort_values('length', inplace = True)\n",
    "    content.sort_values('length', inplace = True)\n",
    "    # Drop cols\n",
    "    topics.drop(['description', 'channel', 'category', 'level', 'language', 'parent', 'has_content', 'length', 'topic_id', 'content_ids'], axis = 1, inplace = True)\n",
    "    content.drop(['description', 'kind', 'language', 'text', 'copyright_holder', 'license', 'length'], axis = 1, inplace = True)\n",
    "    # Reset index\n",
    "    topics.reset_index(drop = True, inplace = True)\n",
    "    content.reset_index(drop = True, inplace = True)\n",
    "    print(' ')\n",
    "    print('-' * 50)\n",
    "    print(f\"topics.shape: {topics.shape}\")\n",
    "    print(f\"content.shape: {content.shape}\")\n",
    "    return topics, content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "780f7851-4862-4a13-b133-30c6d7e3281b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_uns_input(text, cfg):\n",
    "    inputs = cfg.uns_tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors = None, \n",
    "        add_special_tokens = True, \n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype = torch.long)\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71ae651d-c010-4aa9-9f7a-665826c645ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class uns_dataset(Dataset):\n",
    "    def __init__(self, df, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['title'].values\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_uns_input(self.texts[item], self.cfg)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3dbb572-4490-42ca-b93b-25748520b536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sup_input(text, cfg):\n",
    "    inputs = cfg.sup_tokenizer.encode_plus(\n",
    "        text, \n",
    "        return_tensors = None, \n",
    "        add_special_tokens = True, \n",
    "    )\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = torch.tensor(v, dtype = torch.long)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "894cf70f-8d56-4704-869b-6cde5adcb04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class sup_dataset(Dataset):\n",
    "    def __init__(self, df, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.texts = df['text'].values\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, item):\n",
    "        inputs = prepare_sup_input(self.texts[item], self.cfg)\n",
    "        return inputs\n",
    "    #对输入进行张量化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8620fefd-490b-4bf4-bb3a-64af79806fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a47aa1a0-d63f-49f4-9ecc-c2ce1ac8e86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class uns_model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.config = AutoConfig.from_pretrained(cfg.uns_model)\n",
    "        self.model = AutoModel.from_pretrained(cfg.uns_model, config = self.config)\n",
    "        self.pool = MeanPooling()\n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        feature = self.pool(last_hidden_state, inputs['attention_mask'])\n",
    "        return feature\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb451722-c2ce-45e7-9c95-dd199fa20549",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(loader, model, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    for step, inputs in enumerate(tqdm(loader)):\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d639a717-13a0-4754-8f4b-d627e7e3455e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_socre(y_true, y_pred):\n",
    "    y_true = y_true.apply(lambda x: set(x.split()))\n",
    "    y_pred = y_pred.apply(lambda x: set(x.split()))\n",
    "    int_true = np.array([len(x[0] & x[1]) / len(x[0]) for x in zip(y_true, y_pred)])\n",
    "    return round(np.mean(int_true), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d84655f5-9dbe-46fd-9c49-6fbb8d5ebe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inference_set(topics, content, cfg):\n",
    "    # Create lists for training\n",
    "    topics_ids = []\n",
    "    content_ids = []\n",
    "    title1 = []\n",
    "    title2 = []\n",
    "    # Iterate over each topic\n",
    "    for k in tqdm(range(len(topics))):\n",
    "        row = topics.iloc[k]\n",
    "        topics_id = row['id']\n",
    "        topics_title = row['title']\n",
    "        predictions = row['predictions'].split(' ')\n",
    "        for pred in predictions:\n",
    "            content_title = content.loc[pred, 'title']\n",
    "            topics_ids.append(topics_id)\n",
    "            content_ids.append(pred)\n",
    "            title1.append(topics_title)\n",
    "            title2.append(content_title)\n",
    "    # Build training dataset\n",
    "    test = pd.DataFrame(\n",
    "        {'topics_ids': topics_ids, \n",
    "         'content_ids': content_ids, \n",
    "         'title1': title1, \n",
    "         'title2': title2\n",
    "        }\n",
    "    )\n",
    "    # Release memory\n",
    "    del topics_ids, content_ids, title1, title2\n",
    "    gc.collect()\n",
    "    return test  #这里的test集合了correlation中的部分\n",
    "#这里我想的是直接改为进入模型的输入，所以我将predictions这个部分删除了，但是数据集应该没有改对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d476b0c1-aa4b-467e-ae9f-064a00fe6f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(topics, content, cfg):\n",
    "    # Create topics dataset\n",
    "    topics_dataset = uns_dataset(topics, cfg)\n",
    "    # Create content dataset\n",
    "    content_dataset = uns_dataset(content, cfg)\n",
    "    # Create topics and content dataloaders\n",
    "    topics_loader = DataLoader(\n",
    "        topics_dataset, \n",
    "        batch_size = cfg.batch_size, \n",
    "        shuffle = False, \n",
    "        collate_fn = DataCollatorWithPadding(tokenizer = cfg.uns_tokenizer, padding = 'longest'),\n",
    "        num_workers = cfg.num_workers, \n",
    "        pin_memory = True, \n",
    "        drop_last = False\n",
    "    )\n",
    "    content_loader = DataLoader(\n",
    "        content_dataset, \n",
    "        batch_size = cfg.batch_size, \n",
    "        shuffle = False, \n",
    "        collate_fn = DataCollatorWithPadding(tokenizer = cfg.uns_tokenizer, padding = 'longest'),\n",
    "        num_workers = cfg.num_workers, \n",
    "        pin_memory = True, \n",
    "        drop_last = False\n",
    "        )\n",
    "    # Create unsupervised model to extract embeddings\n",
    "    model = uns_model(cfg)\n",
    "    model.to(device)\n",
    "    # Predict topics\n",
    "    topics_preds = get_embeddings(topics_loader, model, device)\n",
    "    content_preds = get_embeddings(content_loader, model, device)\n",
    "    # Transfer predictions to gpu\n",
    "    topics_preds_gpu1 = np.array(topics_preds)\n",
    "    content_preds_gpu1 = np.array(content_preds)\n",
    "    topics_preds_gpu=np.mat(topics_preds_gpu1)\n",
    "    content_preds_gpu=np.mat(content_preds_gpu1)\n",
    "    # Release memory\n",
    "    torch.cuda.empty_cache()\n",
    "    del topics_dataset, content_dataset, topics_loader, content_loader, topics_preds, content_preds\n",
    "    gc.collect()\n",
    "    # KNN model\n",
    "    print(' ')\n",
    "    print('Training KNN model...')\n",
    "    neighbors_model = NearestNeighbors(n_neighbors = cfg.top_n, metric = 'cosine')\n",
    "    neighbors_model.fit(content_preds_gpu)\n",
    "    indices1 = neighbors_model.kneighbors(topics_preds_gpu, return_distance = False)\n",
    "    indices=cp.array(indices1)\n",
    "    predictions = []\n",
    "    for k in range(len(indices)):\n",
    "        pred = indices[k]\n",
    "        p = ' '.join([content.loc[ind, 'id'] for ind in pred.get()])\n",
    "        predictions.append(p)\n",
    "    topics['predictions'] = predictions\n",
    "    # Release memory\n",
    "    del topics_preds_gpu, content_preds_gpu, neighbors_model, predictions, indices, model\n",
    "    gc.collect()\n",
    "    return topics, content \n",
    "#这里直接使用的knn的代码，训练knn模型，这里返回的topics和content我理解的是没有发生变化，是为了得出predictions,但是似乎有些不对"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "17ff74c6-820c-4eb7-9cf3-45105873477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test(test):\n",
    "    test['title1'].fillna(\"Title does not exist\", inplace = True)\n",
    "    test['title2'].fillna(\"Title does not exist\", inplace = True)\n",
    "    # Create feature column\n",
    "    test['text'] = test['title1'] + '[SEP]' + test['title2']#同上，但是这里还加入了correlations的id和title\n",
    "    # Drop titles\n",
    "    test.drop(['title1', 'title2'], axis = 1, inplace = True)\n",
    "    # Sort so inference is faster\n",
    "    test['length'] = test['text'].apply(lambda x: len(x))\n",
    "    test.sort_values('length', inplace = True)\n",
    "    test.drop(['length'], axis = 1, inplace = True)\n",
    "    test.reset_index(drop = True, inplace = True)\n",
    "    gc.collect()\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d0e4cb1-611f-4810-9a73-036a61a554f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_model(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.config = AutoConfig.from_pretrained(cfg.sup_model, output_hidden_states = True)\n",
    "        self.config.hidden_dropout = 0.0\n",
    "        self.config.hidden_dropout_prob = 0.0\n",
    "        self.config.attention_dropout = 0.0\n",
    "        self.config.attention_probs_dropout_prob = 0.0\n",
    "        self.model = AutoModel.from_pretrained(cfg.sup_model, config = self.config)\n",
    "        if self.cfg.gradient_checkpointing:\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "        self.pool = MeanPooling()\n",
    "        self.fc = nn.Linear(self.config.hidden_size, 1)\n",
    "        self._init_weights(self.fc)\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    def feature(self, inputs):\n",
    "        outputs = self.model(**inputs)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        feature = self.pool(last_hidden_state, inputs['attention_mask'])\n",
    "        return feature\n",
    "    def forward(self, inputs):\n",
    "        feature = self.feature(inputs)\n",
    "        output = self.fc(feature)\n",
    "        return output\n",
    "    #CV中的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d702466-4115-4dfc-aaa1-4a0c70399805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_fn(test_loader, model, device):\n",
    "    preds = []\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    tk0 = tqdm(test_loader, total = len(test_loader))\n",
    "    for inputs in tk0:\n",
    "        for k, v in inputs.items():\n",
    "            inputs[k] = v.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(inputs)\n",
    "        preds.append(y_preds.sigmoid().squeeze().to('cpu').numpy().reshape(-1))\n",
    "    predictions = np.concatenate(preds)\n",
    "    return predictions\n",
    "#输出predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb6ea087-42f5-4ddb-884c-439065b48b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(test, cfg):\n",
    "    # Create dataset and loader\n",
    "    test_dataset = sup_dataset(test, cfg)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size = cfg.batch_size, \n",
    "        shuffle = False, \n",
    "        collate_fn = DataCollatorWithPadding(tokenizer = cfg.sup_tokenizer, padding = 'longest'),\n",
    "        num_workers = cfg.num_workers, \n",
    "        pin_memory = True, \n",
    "        drop_last = False\n",
    "    )\n",
    "    # Get model\n",
    "    model = custom_model(cfg)\n",
    "    # Load weights\n",
    "    state = torch.load(\"/root/xlm-roberta-base_fold0_42.pth\", map_location = torch.device('cpu'))\n",
    "    model.load_state_dict(state['model'])\n",
    "    prediction = inference_fn(test_loader, model, device)\n",
    "    # Release memory\n",
    "    torch.cuda.empty_cache()\n",
    "    del test_dataset, test_loader, model, state\n",
    "    gc.collect()\n",
    "    # Use threshold\n",
    "    test['predictions'] = np.where(prediction > cfg.threshold, 1, 0)\n",
    "    test1 = test[test['predictions'] == 1]\n",
    "    test1 = test1.groupby(['topics_ids'])['content_ids'].unique().reset_index()\n",
    "    test1['content_ids'] = test1['content_ids'].apply(lambda x: ' '.join(x))\n",
    "    test1.columns = ['topic_id', 'content_ids']\n",
    "    test0 = pd.Series(test['topics_ids'].unique())#创建字典\n",
    "    test0 = test0[~test0.isin(test1['topic_id'])]\n",
    "    test0 = pd.DataFrame({'topic_id': test0.values, 'content_ids': \"\"})\n",
    "    test_r = pd.concat([test1, test0], axis = 0, ignore_index = True)\n",
    "    test_r.to_csv('submission.csv', index = False)\n",
    "    return test_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39e98b59-3aac-418a-96d1-4c53b65fc9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "--------------------------------------------------\n",
      "topics.shape: (5, 2)\n",
      "content.shape: (154047, 2)\n"
     ]
    }
   ],
   "source": [
    "topics, content = read_data(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "112de59e-8003-4980-bb45-7f2e4289746e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f021baa366e64b91a7385784274ef5c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa06b33d19d64f6bbc462e1d2e2828ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4814 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Training KNN model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics, content = get_neighbors(topics, content, CFG)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4d08330-c680-413a-850e-ba8c9794e253",
   "metadata": {},
   "outputs": [],
   "source": [
    "content.set_index('id', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3e191e5-92f5-4889-8e15-f0e2de8963f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f54804c455c4c54871c5c00df997292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test = build_inference_set(topics, content, CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "07ce793e-d437-4ef9-9643-77d7f1da7fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = preprocess_test(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98029ef1-fc21-4bc3-89cc-331c760fd64e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed2875e9d714a64831c2d99d7e0e5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_id</th>\n",
       "      <th>content_ids</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t_00004da3a1b2</td>\n",
       "      <td>c_b3c90a7fdca0 c_3680d0d849f4 c_a363b56c832a c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t_00068291e9a4</td>\n",
       "      <td>c_48bd37bb95b4 c_a9a72a49f799 c_2407ad9d7ed0 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t_00069b63a70a</td>\n",
       "      <td>c_9b9d563abd43 c_996d765719c0 c_e7a019ccc58c c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t_0006d41a73a8</td>\n",
       "      <td>c_0dc6db0e2f25 c_fdb706b17f0a c_af08c6756929 c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t_4054df11a74e</td>\n",
       "      <td>c_d4b292f4f273 c_81d28e4165b0 c_953ab4ac76fa c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         topic_id                                        content_ids\n",
       "0  t_00004da3a1b2  c_b3c90a7fdca0 c_3680d0d849f4 c_a363b56c832a c...\n",
       "1  t_00068291e9a4  c_48bd37bb95b4 c_a9a72a49f799 c_2407ad9d7ed0 c...\n",
       "2  t_00069b63a70a  c_9b9d563abd43 c_996d765719c0 c_e7a019ccc58c c...\n",
       "3  t_0006d41a73a8  c_0dc6db0e2f25 c_fdb706b17f0a c_af08c6756929 c...\n",
       "4  t_4054df11a74e  c_d4b292f4f273 c_81d28e4165b0 c_953ab4ac76fa c..."
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_r = inference(test, CFG)\n",
    "test_r.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "abefddd8",
   "metadata": {},
   "source": [
    "这里不清楚inferences里面具体要输入什么"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
